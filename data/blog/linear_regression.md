---
title: 'Linear Regression'
date: 2023-08-07
tags: ['Supervised Learning', 'Machine Learning', 'Linear Regression']
summary: 'machine learning notes'
---

## Table of Contents

<TOCInline toc={props.toc} exclude="Table of Contents" />

# Regression

Regresyon, bir makine Ã¶ÄŸrenimi yÃ¶ntemi olarak kullanÄ±lan bir sÃ¼reÃ§tir ve bir baÄŸÄ±mlÄ± deÄŸiÅŸkenin (hedef deÄŸiÅŸken) bir veya daha fazla baÄŸÄ±msÄ±z deÄŸiÅŸkenle (girdi deÄŸiÅŸkenleri) iliÅŸkisini modellemeyi amaÃ§lar. Regresyon analizi, baÄŸÄ±mlÄ± deÄŸiÅŸkenin sÃ¼rekli bir deÄŸer almasÄ± durumunda uygulanÄ±r. Bu yÃ¶ntem, baÄŸÄ±msÄ±z deÄŸiÅŸkenlerin deÄŸerlerine dayanarak baÄŸÄ±mlÄ± deÄŸiÅŸkenin tahmin edilmesini saÄŸlamayÄ± hedefler. Regresyon modelleri, veri noktalarÄ±nÄ±n bir fonksiyon veya eÄŸri Ã¼zerinde en iyi ÅŸekilde uymasÄ±nÄ± saÄŸlar. Regresyon analizi, tahmin yapma, trendleri belirleme, iliÅŸkileri anlama gibi birÃ§ok uygulama alanÄ±nda kullanÄ±lan Ã¶nemli bir yÃ¶ntemdir.

## Tek DeÄŸiÅŸkenli Linear Regresyon

Tek deÄŸiÅŸkenli regresyon, sadece bir baÄŸÄ±msÄ±z deÄŸiÅŸkenin (girdi deÄŸiÅŸkeni) baÄŸÄ±mlÄ± deÄŸiÅŸkenle (hedef deÄŸiÅŸken) iliÅŸkisini modellemek iÃ§in kullanÄ±lan bir regresyon yÃ¶ntemidir. Bu yÃ¶ntemde, baÄŸÄ±mlÄ± deÄŸiÅŸkenin sÃ¼rekli bir deÄŸer almasÄ± durumunda uygulanÄ±r. Tek deÄŸiÅŸkenli regresyon analizi, baÄŸÄ±msÄ±z deÄŸiÅŸkenin deÄŸerine dayanarak baÄŸÄ±mlÄ± deÄŸiÅŸkenin tahmin edilmesini saÄŸlamayÄ± amaÃ§lar. Bu analizde, veri noktalarÄ±nÄ±n bir doÄŸru Ã¼zerinde en iyi ÅŸekilde uymasÄ± hedeflenir. Tek deÄŸiÅŸkenli regresyon, iliÅŸkiyi anlamak, tahmin yapmak veya trendleri belirlemek gibi birÃ§ok uygulama alanÄ±nda kullanÄ±lan temel bir regresyon yÃ¶ntemidir.

### Linear Regresyon Modeli

Diyelim ev boyuna gÃ¶re fiyatÄ±nÄ± tahmin etmek istiyoruz. Burada X ekseni evin boyutu (size feetÂ²), y ekseni de evin fiyatÄ±nÄ± 1000$ cinsinden temsil eden bir grafik var.

![regg](/static/regg.png)

Burada gÃ¶rÃ¼nen her bir nokta, satÄ±lan evlerin boyutunu ve fiyatÄ±nÄ± temsil eden verilerdir. Bu grafik sayesinde bir evin yaklaÅŸÄ±k deÄŸerini hesaplayabiliriz. 1250 fitÂ²'lik bir evin fiyatÄ±nÄ± Ã¶ÄŸrenmek iÃ§in veri setine dayalÄ± olarak doÄŸrusal regresyon modeli kullanmak, bizi yaklaÅŸÄ±k bir deÄŸere ulaÅŸtÄ±racaktÄ±r. Model, verilerimize gÃ¶re dÃ¼z bir Ã§izgi oluÅŸturur.

![regg](/static/reg_line.png)

Bu doÄŸrusal Ã§izgiye gÃ¶re, 1250 fitÂ²lik bir evin fiyatÄ± yaklaÅŸÄ±k 220 bin dolar olacaktÄ±r. Buna gÃ¶zetimli Ã¶ÄŸrenme denir, Ã§Ã¼nkÃ¼ modeli oluÅŸturmak iÃ§in cevaplarÄ± verilmiÅŸ (ev fiyatÄ±) Ã¶rneklerden oluÅŸan bir veri seti kullandÄ±k. Bu veri seti, hem evin boyutunu hem de evin fiyatÄ±nÄ± (cevaplarÄ±nÄ±) iÃ§erir. BÃ¶ylece modelimiz, evin boyutuna gÃ¶re ev fiyatÄ±nÄ± yaklaÅŸÄ±k bir deÄŸerle tahmin etmeyi Ã¶ÄŸrenir.

Regresyon modeli, 200 bin, 1.5, -33.2 gibi sÃ¼rekli sayÄ±larÄ± tahmin eden gÃ¶zetimli Ã¶ÄŸrenme problemlerini Ã§Ã¶zer. Burada Ã¶nemli olan regresyonun kategorik veriler yerine sÃ¼rekli verilerle ilgilendiÄŸidir.

**Notasyon**

| **index ($i$)** | **Size in feetÂ² ($ğ±$)** | **Price in 1000 ($ğ²$)** | **$(ğ‘¥^i ,ğ‘¦^i)$**          |
| --------------- | ----------------------- | ----------------------- | ------------------------- |
| 1               | 2104 $(ğ±Â¹)$             | 400 $(ğ²Â¹)$              | $(ğ±Â¹, ğ²Â¹)$ = (2104, 400)  |
| 2               | 1416 $(ğ±Â²)$             | 232 $(ğ²Â²)$              | $(ğ±Â², ğ²Â²)$ = 1416, 232)   |
| 3               | 1534 $(ğ±Â³)$             | 315 $(ğ²Â³)$              | $(ğ±Â³, ğ²Â³)$ = (1534, 315)  |
| 4               | 852 $(ğ±â´)$              | 178 $(ğ²â´)$              | $(ğ±â´,ğ²â´)$ = (852, 178)    |
| â€¦..             | â€¦..                     | â€¦..                     | â€¦..                       |
| 47 ($m$)        | 3210 $(ğ±â´â·)$            | 870 $(ğ²â´â·)$             | $(ğ±â´â·, ğ²â´â·)$ = (3210,870) |

| **Notasyon** | **AÃ§Ä±klama**                                                    | **Ã–rnek**                | **Python variable** |
| ------------ | --------------------------------------------------------------- | ------------------------ | ------------------- |
| $ğ±$          | Training Example feature variable or input variable             | (ğ±Â³) = 1534              | x_train             |
| $ğ²$          | Training Example targets variable or output variable            | (ğ²Â³) = 315               | y_train             |
| $m$          | Number of training examples                                     | 47                       | m                   |
| $(ğ‘¥^ğ‘– ,ğ‘¦^ğ‘–)$ | ğ‘–ğ‘¡â„ Training Example                                            | (ğ±Â³ , ğ²Â³) = (1534, 315 ) | x_i,Â y_i            |
| $ğ‘¤$          | parameter: weight                                               |                          | w                   |
| $ğ‘$          | parameter: bias                                                 |                          | b                   |
| $Å·$          | estimation of the target variable (y-hat)                       | ğ² = 400 Å· = 390.9        | y_hat               |
| $ğ‘“ğ‘¤,ğ‘(ğ‘¥^ğ‘–)$  | The result of the model evaluation atÂ ğ‘¥(ğ‘–)parameterized byÂ ğ‘¤,ğ‘: |                          | f_wb                |
| $ğ‘$          | scalar, non bold                                                |                          |                     |
| $ğš$          | vector, bold                                                    |                          |                     |

![learning_Alg](/static/learning_Algorithm.png)

1. EÄŸitim Seti, yani eÄŸitim seti, iki bileÅŸenden oluÅŸmaktadÄ±r: Ã–zellikler veya girdiler (x - evin boyutu) ve Hedefler veya Ã§Ä±ktÄ±lar (y - evin fiyatÄ±).
2. Modeli eÄŸitmek iÃ§in eÄŸitim seti kullanÄ±larak hem girdi Ã¶zellikleri (Ã¶zellikler) hem de Ã§Ä±ktÄ± hedefleri (hedefler), Ã¶ÄŸrenme algoritmasÄ±na beslenir.

3. Daha sonra gÃ¶zetimli Ã¶ÄŸrenme algoritmasÄ±, belirli bir iÅŸlevi Ã¼retir. Bu iÅŸlev genellikle $f$ ile gÃ¶sterilir. $f$ fonksiyonunun amacÄ±, yeni bir x deÄŸeri almak ve tahmini bir Ã§Ä±ktÄ± Ã¼retmektir. Bu tahmini Ã§Ä±ktÄ±ya $Å·$ (y-hat) adÄ± verilir. $Å·$, aslÄ±nda gerÃ§ek $y$ deÄŸeri iÃ§in bir tahmin veya yaklaÅŸÄ±k bir deÄŸerdir.

Ã–rneÄŸin, Ev GeniÅŸliÄŸi Ã¶zelliÄŸini modele vererek tahmini ev fiyatÄ± Ã§Ä±ktÄ±sÄ±nÄ± elde ediyoruz.

**$f$ fonksiyonunu nasÄ±l ifade etmeliyiz?**

$$ğ‘“ğ‘¤,ğ‘(ğ‘¥) = wx + b $$

w ve b sayÄ±larÄ± iÃ§in seÃ§ilen deÄŸerler, x girdilerini kullanarak Å· Ã§Ä±ktÄ±sÄ±nÄ± tahmin etmek iÃ§in kullanÄ±lÄ±r. (w ve b parametreleri aÅŸaÄŸÄ±da aÃ§Ä±klanmÄ±ÅŸtÄ±r)

![f_fonk](/static/f_fonk.png)

Algoritma bu veriden Ã¶ÄŸrenir ve en iyi uyan Ã§izgiyi oluÅŸturmaya Ã§alÄ±ÅŸÄ±r. Lineer fonksiyon $f_w, b(x) = wx + b$ ile ifade edilir. Bu fonksiyonun amacÄ±, buradaki $f(x)$ fonksiyonu Ã§izgisini kullanarak y deÄŸerini tahmin etmektir ($y â†’ Å·$). Ä°ÅŸte lineer regresyonun temeli budur. Bundan sonraki adÄ±m, maliyet (cost) fonksiyonunu oluÅŸturmaktÄ±r.

### Linear Regresyon - Cost Function

Lineer Regresyon uygulamak iÃ§in yapÄ±lmasÄ± gereken ilk adÄ±m, maliyet (cost) fonksiyonunu tanÄ±mlamaktÄ±r. Bu Cost fonksiyonu, modelin ne kadar baÅŸarÄ±lÄ± olduÄŸunu bize sÃ¶yler ve daha iyi sonuÃ§lar elde etmek iÃ§in bu fonksiyonu minimize etmeye Ã§alÄ±ÅŸÄ±rÄ±z.

**Linear Fonksiyon**
EÄŸitim setimiz (training set), girdi Ã¶zellikleri (feature) x ve Ã§Ä±ktÄ± hedefleri (target) y iÃ§ermektedir.

![training_set](/static/training_set.png)

Bu eÄŸitim verisini (training data) beslemeyi amaÃ§ladÄ±ÄŸÄ±mÄ±z (fit) model, aÅŸaÄŸÄ±daki gibi gÃ¶sterilen bir lineer fonksiyondur:

Model : $$ f w,b(x) = wx + b $$

Makine Ã¶ÄŸrenmesinde bir modelin parametreleri, modeli geliÅŸtirmek iÃ§in eÄŸitim sÃ¼recinde ayarlayabileceÄŸimiz deÄŸiÅŸkenlerdir. Bu parametreler $w$ ve $b$ olarak adlandÄ±rÄ±lÄ±r.

$b$ = katsayÄ±lar (coefficients)

$w$ = AÄŸÄ±rlÄ±klar veya sabit (weights or constant) (eÄŸim)

Åimdi $b$ ve $w$'nin ne iÅŸe yaradÄ±ÄŸÄ±na bakalÄ±m.

**$b$ ve $w$ Nedir?**

$w$ ve $b$ iÃ§in seÃ§tiÄŸimiz deÄŸerlere baÄŸlÄ± olarak farklÄ± bir $f(x)$ fonksiyonu elde edebiliriz. Bu da fonksiyonun grafikte farklÄ± Ã§izgiler Ã§izmesine neden olur.

**Ã¶rnek 1**

![f_fonk_1](/static/f_fonk_1.png)

Burada $w=0$ ve $b=1.5$ olduÄŸunda, $f$ fonksiyonu yatay bir Ã§izgi olur. Ã‡Ã¼nkÃ¼ $w$ deÄŸeri 0 olduÄŸu iÃ§in $x$ ile Ã§arpÄ±mÄ± da 0 olacaktÄ±r. Yani $f$, her zaman sabit bir deÄŸeri temsil eder. Her zaman $Å·$ (tahmini) deÄŸeri olarak 1.5 deÄŸerini alÄ±r.

**Ã¶rnek 2**

![f_fonk_2](/static/f_fonk_2.png)

$w=0.5$ ve $b=0$ olduÄŸunda:

$x = 0$ ise:
$f(0)$ = $f(x)= 0,5 x 0 + 0$ = 0 ---> 1.nokta oluÅŸur.

$x = 1$ ise:
$f(1)$ = $f(x)= 0,5 x 1 + 0$ = 0.5 ---> 2.nokta oluÅŸur.

$x=2$ ise:
$f(2)$ = $f(x)= 0,5 x 2 + 0$ = 1 ---> 3.nokta oluÅŸur.

$x=3$ ise:
$f(3)$ = $f(x)= 0,5 x 3 + 0$ = 1.5 ---> 4.nokta oluÅŸur.

**Ã–rnek 3**

![f_fonk_3](/static/f_fonk_3.png)

$w=0.5$ ve $b=1$ olduÄŸunda:

$x = 0$ ise:
$f(0)$ = $f(x)= 0,5 x 0 + 1$ = 1 ---> 1.nokta oluÅŸur.

$x = 1$ ise:
$f(1)$ = $f(x)= 0,5 x 1 + 1$ = 1,5 ---> 2.nokta oluÅŸur.

$x=2$ ise:
$f(2)$ = $f(x)= 0,5 x 2 + 1$ = 2 ---> 3.nokta oluÅŸur.

$x=3$ ise:
$f(3)$ = $f(x)= 0,5 x 3 + 1$ = 2,5 ---> 4.nokta oluÅŸur.

![y_hat](/static/y_hat.png)

$f(x)$ fonksiyonundan elde edilen dÃ¼z Ã§izgi, veriye uygun olmalÄ±dÄ±r. NotasyonlarÄ± hatÄ±rlatmak adÄ±na, bir eÄŸitim Ã¶rneÄŸi $(x^i, y^i)$ olarak tanÄ±mlanÄ±r. $x^i$ olarak belirtilen bu giriÅŸ, $f$ fonksiyonu tarafÄ±ndan $y$'nin tahmini bir deÄŸeri Ã¼retir. Bu Ã¼retilen deÄŸer $Å·$ olarak gÃ¶sterilir.

Åimdi sorulmasÄ± gereken soru ÅŸudur: $Å·$ deÄŸerini bulmak iÃ§in $w$ ve $b$ deÄŸerlerini nasÄ±l ayarlamalÄ±yÄ±z, bÃ¶ylece bÃ¼tÃ¼n $x^i$ ve $y^i$ deÄŸerleri iÃ§in en yakÄ±n sonucu tahmin edebiliriz? Bu sorunun cevabÄ±nÄ± bulmak iÃ§in Ã¶nce bir Ã§izginin eÄŸitim verisine ne kadar uyduÄŸunu hesaplamayÄ± bilmeliyiz. Bu amaÃ§la, maliyet (cost) fonksiyonunu oluÅŸturacaÄŸÄ±z.

**Cost (Maliyet) Fonksiyonu**

$$ MSE=j(w,b) = {1\over2m} \sum\_{i=1}^{m}{(Å·^i - y^i)^2} $$

Bir makine Ã¶ÄŸrenmesi modelini eÄŸitirken, modelin performansÄ±nÄ± Ã¶lÃ§mek iÃ§in bir maliyet fonksiyonu kullanÄ±lÄ±r. Bu fonksiyon, modelin tahminleri ile gerÃ§ek deÄŸerler arasÄ±ndaki farklarÄ± hesaplayarak, modelin ne kadar doÄŸru tahmin yaptÄ±ÄŸÄ±nÄ± belirler.

Maliyet fonksiyonu, tahmini deÄŸerden gerÃ§ek deÄŸeri Ã§Ä±kararak bir hata (error) deÄŸeri elde eder. Bu hata genellikle karesi alÄ±narak hesaplanÄ±r. Her bir Ã¶rnek iÃ§in hata karesini hesaplamak ve toplam hatayÄ± hesaplamak iÃ§in Ã¶rneklerin sÄ±rasÄ±na gÃ¶re bir dÃ¶ngÃ¼ oluÅŸturulur. Bu nedenle maliyet fonksiyonunda hata karelerini hesaplamak iÃ§in MSE (Mean Squared Error) metriÄŸi kullanÄ±lÄ±r. MSE yerine mse, rmse, mae, mape gibi metrikler de kullanÄ±labilir. Bu baÄŸlamda, burada MSE metriÄŸini kullanmaktayÄ±z.

EÄŸitim seti bÃ¼yÃ¼dÃ¼kÃ§e, maliyet fonksiyonu daha bÃ¼yÃ¼k deÄŸerler hesaplayabilir. Bu nedenle, hata karelerinin ortalamasÄ±nÄ± alarak hesaplanan bir maliyet fonksiyonu kullanmak daha uygundur. BÃ¶ylece, eÄŸitim setindeki Ã¶rnek sayÄ±sÄ± arttÄ±kÃ§a, maliyet fonksiyonu otomatik olarak artmaz ve daha istikrarlÄ± bir performans Ã¶lÃ§Ã¼mÃ¼ gerÃ§ekleÅŸtirilir.

#### Cost Function Sezileri gÃ¼Ã§lendirme

Cost fonksiyonunun bir model iÃ§in en iyi parametreleri nasÄ±l bulunduÄŸunu bir Ã¶rnek Ã¼zerinden inceleyecek olursak:

Model: $f_{w,b}(x) = wx + b$

Parametreler: $w, b$

Cost fonksiyonu: $MSE = J(w,b) = \frac{1}{2m} \sum_{i=1}^{m}{(\hat{y}^i - y^i)^2}$

Hedef: $J(w,b)$'yi minimize etmek

DoÄŸrusal modelimiz olan $f_{w,b}(x) = wx + b$, eÄŸitim veri setimize tam olarak uyan bir dÃ¼z Ã§izgi elde etmek iÃ§in $w$ ve $b$ parametreleri iÃ§in en uygun deÄŸerleri seÃ§memiz gerekmektedir. Bu ÅŸekilde farklÄ± $w$ ve $b$ deÄŸerlerini kullanarak farklÄ± Ã§izgiler elde edebiliriz. Daha sonra seÃ§ilen $w$ ve $b$ deÄŸerleri iÃ§in $J$ cost fonksiyonunu ($J(w,b)$) oluÅŸturarak modelin tahmin ettiÄŸi deÄŸer ile gerÃ§ek deÄŸer arasÄ±ndaki farkÄ± Ã¶lÃ§eriz. Son olarak, $J(w,b)$ fonksiyonunu minimize ederek seÃ§ilen $w$ ve $b$ deÄŸerlerinin eÄŸitim verisine ne kadar iyi uyduÄŸunu Ã¶lÃ§eriz. Bu sayede, doÄŸrusal modelimiz en iyi performansÄ±nÄ± gÃ¶sterir ve en uygun $w$ ve $b$ deÄŸerleri ile eÄŸitim setine en uygun Ã§izgiyi Ã§izebilir.

**BasitleÅŸtirilmiÅŸ Ã–rnek**

![basit](/static/basit.png)

Bu basitleÅŸtirilmiÅŸ Ã¶rnekte yukarÄ±daki Ã¶rneÄŸin aynÄ±sÄ±nÄ± aldÄ±k, ancak basitleÅŸtirmek amacÄ±yla $b$ parametresini denklemden Ã§Ä±kartarak (0'a eÅŸitleyerek) $f_w(x) = wx$ modelini oluÅŸturduk. Yani Ã§izgimiz artÄ±k orijinden geÃ§iyor, Ã§Ã¼nkÃ¼ $x=0$ olduÄŸunda $f(x)$ de 0 olacaktÄ±r. Bu basitleÅŸtirilmiÅŸ modelle amacÄ±mÄ±z, $w$ iÃ§in $J(w)$ fonksiyonunu kÃ¼Ã§Ã¼ltecek bir deÄŸeri bulmaktÄ±r.

model : $fw(x) = wx$

cost function : $j(w) = {1\over2m}  \sum_{i=1}^{m}{(Å·^i - y^i)^2}$

hedef : $minimize j(w)$

$w (eÄŸim)= 1$

$j(1) = ?$

![orn](/static/orn.png)

Tabloya gÃ¶re Ã¼Ã§ eÄŸitim Ã¶rneÄŸi sÄ±rasÄ±yla (1,1), (2,2), (3,3) olarak gÃ¶sterilmiÅŸtir. Bu veri seti Ã¶zelinde $j$ cost fonksiyonunu $w = 1$ iÃ§in hesaplayacaÄŸÄ±z, yani $j(1)$. HatÄ±rlarsanÄ±z, $j$ cost fonksiyonu gerÃ§ek deÄŸerden tahmin edileni Ã§Ä±karÄ±p karesini alÄ±r ve her birini toplar, sonrasÄ±nda $\frac{1}{2m}$'ye bÃ¶ler. Buna "hata kareler fonksiyonu" denir. Ã–rneÄŸimizde de bu formÃ¼lÃ¼ her bir eÄŸitim verisi iÃ§in uygulayacaÄŸÄ±z, yani $i=1$'den $m$'e kadar. FormÃ¼l uygulandÄ±ktan sonra, bu veri seti Ã¶zelinde $w=1$ iken $j(1) = 0$ Ã§Ä±ktÄ±. Soldaki grafiÄŸimize Ã§Ä±kan sonucu iÅŸaretledik.

Ya $w$ deÄŸeri 1 yerine 0.5 olursa, o zaman bu grafik nasÄ±l gÃ¶zÃ¼kÃ¼rdÃ¼?

$w$ (eÄŸim) = 0.5

$j(0.5) = ?$

![orn2](/static/orn2.png)

AynÄ± eÄŸitim veri Ã¶rnekleri iÃ§in bu sefer $w = 0.5$ iÃ§in $j$ cost fonksiyonunu hesaplayacaÄŸÄ±z, yani $j(0.5)$ deÄŸerini bulacaÄŸÄ±z. Ä°lk Ã¶rnekte $x=1$ olduÄŸunda $f(x)=0.5$ deÄŸerini alÄ±r. Bu nedenle ilk Ã¶rneÄŸin hata karesi $(0.5-1)^2$ olarak hesaplanÄ±r. Ä°kinci Ã¶rnekte $x=2$ olduÄŸunda $f(x)=1$, bu Ã¶rneÄŸin hata karesi ise $(1-2)^2$ olarak hesaplanÄ±r. ÃœÃ§Ã¼ncÃ¼ Ã¶rnekte $x=3$ olduÄŸunda $f(x)=1.5$, bu Ã¶rneÄŸin hatasÄ± ise $(1.5-3)^2$ olarak hesaplanÄ±r. TÃ¼m eÄŸitim Ã¶rnekleri hesaplandÄ±ktan sonra, bu deÄŸerler toplanÄ±p $1/2m$ ile Ã§arpÄ±lÄ±r. BÃ¶ylece $j(0.5) = 0.583$ olarak hesaplanÄ±r. Ã‡Ä±kan sonucumuzu soldaki grafikteki yerine yazÄ±yoruz.

$w=0$ desek nasÄ±l olur? $w=0$ iÃ§in $f$ ve $J$ fonksiyonlarÄ±nÄ±n grafiÄŸi nasÄ±l gÃ¶zÃ¼kÃ¼rdÃ¼?

$w=0$

$f(0) = ?$

![orn3](/static/orn3.png)

AynÄ± eÄŸitim veri Ã¶rnekleri iÃ§in bu sefer $j$ cost fonksiyonunu $w = 0$ iÃ§in hesaplayacaÄŸÄ±z, yani $j(0)$. Burada $f(x)$ tam olarak yatay Ã§izgi olacaktÄ±r. Bu sebeple bÃ¼tÃ¼n tahmin edilen deÄŸerler 0'a eÅŸit olacaktÄ±r, yani $f(x) = 0$. $j$ fonksiyonu ${1\over2m} \times (1^2 + 2^2 + 3^2)$ olacaktÄ±r. BÃ¼tÃ¼n iÅŸlemlerden sonra Ã§Ä±kan sonuÃ§ ise $2.33$ yapar. Yani $j(0) = 2.33$.

FarklÄ± $w$ deÄŸerleri iÃ§in cost fonksiyonunu hesaplayabilir ve grafiÄŸini Ã§izebiliriz. BazÄ± deÄŸerleri hesaplayarak, $j$ fonksiyonunun nasÄ±l gÃ¶zÃ¼keceÄŸini gÃ¶rebiliriz.

![orn4](/static/orn4.png)

YukarÄ±daki Ã¶rneklerde seÃ§tiÄŸimiz farklÄ± $w$ deÄŸerleri gibi daha farklÄ± $w$ deÄŸerleri seÃ§erek yukarÄ±daki grafiÄŸi oluÅŸturabiliriz. Her bir nokta, farklÄ± bir $f(x)$ Ã§izgisini temsil etmektedir. SonuÃ§ olarak Ã§Ä±kan bu "u" ÅŸeklindeki Ã§izgimizin en dÃ¼ÅŸÃ¼k deÄŸeri $j(1)$ noktasÄ±dÄ±r. AmacÄ±mÄ±z $min j(w)$ deÄŸerini bulmaktÄ± ve bÃ¶ylelikle en uygun $w$ deÄŸerini elde ettik.

Bu Ã¶rnekte daha kolay anlayabilmek iÃ§in $b$ parametresi Ã§Ä±kartÄ±lmÄ±ÅŸ ve Ã¶rnek basitleÅŸtirilmiÅŸtir.

#### Cost Function GÃ¶rselleÅŸtirme

YukarÄ±daki basitleÅŸtirilmiÅŸ Ã¶rneÄŸimizde sadece $w$ parametresi alÄ±narak $b = 0$'a eÅŸitlenmiÅŸ ve 2 boyutlu gÃ¶rsel oluÅŸturulmuÅŸtu. Bu gÃ¶rselleÅŸtirmede $w$ ve $b$ parametrelerini kullanarak nasÄ±l bir gÃ¶rsel oluÅŸturulduÄŸunu ve asÄ±l gÃ¶rselin nasÄ±l okunduÄŸunu aÃ§Ä±klayacaÄŸÄ±z. Ancak bu tÃ¼r gÃ¶rselleÅŸtirmeler bÃ¼yÃ¼k eÄŸitim setlerinde etkili olmayabilir ve bu tÃ¼r durumlarda Gradyan Ä°niÅŸi gibi diÄŸer yÃ¶ntemler kullanÄ±lmaktadÄ±r. Gradyan Ä°niÅŸi, en iyi $w$ ve $b$ deÄŸerlerini otomatik olarak bularak $J$ cost fonksiyonunu minimize etmeye yardÄ±mcÄ± olur.

![img1](/static/img1.png)

Bu metindeki 3 boyutlu dÃ¼zlem grafiÄŸi, $w$ ve $b$ olarak ayarlanmÄ±ÅŸ eksenlerle gÃ¶sterilir. $w$ ve $b$ deÄŸerleri deÄŸiÅŸtikÃ§e, modelin iki parametresi olan $J(w,b)$ fonksiyonu iÃ§in farklÄ± deÄŸerler elde edilir. Her nokta, belirli bir $w$ ve $b$ deÄŸeri iÃ§in $J$ noktasÄ±nÄ± temsil eder. Ã–rneÄŸin, $w=-10$ ve $b=-15$ ise, bu noktanÄ±n yÃ¼zeyden uzaklÄ±ÄŸÄ± $J(-10, -15)$ noktasÄ±na karÅŸÄ±lÄ±k gelen yÃ¼kseklikle Ã¶lÃ§Ã¼lÃ¼r.

$J$ fonksiyonunun daha kullanÄ±ÅŸlÄ± ve anlaÅŸÄ±lÄ±r bir gÃ¶rselleÅŸtirmesi de mevcuttur.

![img3](/static/img3.png)

SaÄŸ Ã¼stte gÃ¶rdÃ¼ÄŸÃ¼nÃ¼z ÅŸey, aynÄ± $J$ fonksiyonunun kontÃ¼r grafiÄŸidir. Dikey eksen $b$, yatay eksen ise $w$ deÄŸerlerini temsil eder. KontÃ¼r grafiÄŸi, 3 boyutlu yÃ¼zeyin farklÄ± yÃ¼ksekliklerini gÃ¶sterir. Her bir elips, 3 boyutlu yÃ¼zeyin merkez noktasÄ±nÄ±n aynÄ± yÃ¼ksekliÄŸine denk gelir. BaÅŸka bir deyiÅŸle, bu elipslerin her biri aynÄ± $J$ deÄŸerine karÅŸÄ±lÄ±k gelir.

KontÃ¼r grafiÄŸi oluÅŸturmak iÃ§in, 3 boyutlu grafiÄŸi yatay olarak kesiyoruz. Bu ÅŸekilde aynÄ± yÃ¼ksekliÄŸe sahip noktalarÄ± elde ederiz. Bu kesiklerden her biri, elipslerden birini temsil eder.

Mavi, turuncu ve yeÅŸil oklarla iÅŸaretlenen noktalarÄ±n hepsi, farklÄ± $w$ ve $b$ deÄŸerlerine sahip olmalarÄ±na raÄŸmen aynÄ± $J$ deÄŸerine sahiptir. Ancak bu Ã¼Ã§ nokta, ev fiyatÄ± tahminleri iÃ§in kÃ¶tÃ¼ sonuÃ§lar verir. Kasenin dibi, $J$ fonksiyonunun en kÃ¼Ã§Ã¼k deÄŸere sahip olduÄŸu noktadÄ±r ve bu nokta eÅŸ merkezli elipslerin tam ortasÄ±nda yer alÄ±r.

SaÄŸ Ã¼stteki gÃ¶rseldeki $J$ fonksiyonunun en kÃ¼Ã§Ã¼k deÄŸeri, halkalarÄ±n tam ortasÄ±ndaki noktaya karÅŸÄ±lÄ±k gelir.

**GÃ¶rselleÅŸtirme Ã–rnekleri**

**Ã–rnek 1**

$f(x) = -0.15x + 800$

![12d](/static/12d.png)

Grafikte belirli bir nokta gÃ¶sterilmekte ve bu nokta $w$ ve $b$ parametrelerinin deÄŸerlerini temsil etmektedir. $w$ yaklaÅŸÄ±k olarak $-0.15$ ve $b$ yaklaÅŸÄ±k olarak $800$ olarak seÃ§ilmiÅŸtir. Bu parametrelerle oluÅŸturulan $f(x)$ fonksiyonu sol tarafta Ã§izilmiÅŸtir. Ancak bu fonksiyonun veri setine tam olarak uymadÄ±ÄŸÄ± gÃ¶rÃ¼lmektedir. Ã‡Ã¼nkÃ¼ veri noktalarÄ±nÄ±n Ã§oÄŸu bu fonksiyonun uzakÄ±nda kalmaktadÄ±r. Bu nedenle $J$ grafiÄŸindeki cost deÄŸeri oldukÃ§a yÃ¼ksek ve minimum deÄŸere yakÄ±n deÄŸildir. FarklÄ± $w$ ve $b$ deÄŸerleri seÃ§ilseydi, cost deÄŸeri daha dÃ¼ÅŸÃ¼k olabilirdi.

**Ã–rnek 2**

$f(x) = 0x + 360$

![13d](/static/13d.png)

Bu Ã¶rnek, veriye tam olarak uymasa da Ã–rnek 1'den daha iyi bir seÃ§im olabilir. "KÃ¶tÃ¼nÃ¼n iyisi" olarak adlandÄ±rÄ±labilir. Bu nokta, $w$ ve $b$ parametrelerinin deÄŸerlerini temsil etmektedir. $w=0$ ve $b=360$ olarak seÃ§ilmiÅŸtir. Bu parametrelerle oluÅŸturulan $f(x)$ fonksiyonu sol tarafta Ã§izilmiÅŸtir. Bu fonksiyon yatay bir Ã§izgidir Ã§Ã¼nkÃ¼ $f(x) = 0 \cdot x + 360$ formÃ¼lÃ¼ne sahiptir, yani $w=0$ olduÄŸu iÃ§in.

**Ã–rnek 3**

$f(x) = -0.15x + 500$

![14d](/static/14d.png)

Bu Ã¶rnekte farklÄ± $w$ ve $b$ deÄŸerleri kullanÄ±larak oluÅŸturulan baÅŸka bir $f(x)$ Ã§izgisi gÃ¶rÃ¼lmektedir. Ancak bu Ã§izginin veriye uygun olmadÄ±ÄŸÄ± aÃ§Ä±ktÄ±r. Ã–nceki Ã¶rneÄŸe gÃ¶re daha kÃ¶tÃ¼ bir seÃ§imdir. Cost deÄŸeri oldukÃ§a yÃ¼ksektir ve minimuma ulaÅŸmaktan uzaktÄ±r. Minimum nokta, en kÃ¼Ã§Ã¼k elipsin ortasÄ±nda yer almalÄ±dÄ±r.

**Ã–rnek 3 (en uygun)**

$f(x) = 0.13x + 71$

![15d](/static/15d.png)

Son Ã¶rnek olarak, sol taraftaki $f(x)$ fonksiyonunun veriye Ã§ok daha uygun olduÄŸu gÃ¶rÃ¼lmektedir. SaÄŸdaki grafikte, cost (hata) fonksiyonunu temsil eden noktanÄ±n en kÃ¼Ã§Ã¼k elipsin merkezine yakÄ±n olduÄŸu gÃ¶rÃ¼lmektedir. Tam olarak minimum deÄŸeri ifade etmese de oldukÃ§a yakÄ±ndÄ±r. Bu $w$ ve $b$ deÄŸerleri ile sol taraftaki $f(x)$ Ã§izgisi oluÅŸturulmuÅŸtur. Her veri noktasÄ± iÃ§in tahmin edilen deÄŸer ile gerÃ§ek deÄŸer arasÄ±ndaki dikey mesafeyi Ã¶lÃ§erek hatayÄ± hesaplayabiliriz. BÃ¼tÃ¼n veri noktalarÄ± iÃ§in hata karelerinin toplamÄ±nÄ± hesapladÄ±ÄŸÄ±mÄ±zda, olasÄ± bÃ¼tÃ¼n dÃ¼z Ã§izgiler arasÄ±nda en dÃ¼ÅŸÃ¼k hataya sahip olanÄ± seÃ§miÅŸ oluruz.

Lineer regresyonda en iyi $w$ ve $b$ deÄŸerlerini bulmak iÃ§in kontÃ¼r grafiÄŸini okumak yeterli deÄŸildir ve daha karmaÅŸÄ±k makine Ã¶ÄŸrenmesi modellerinde iÅŸe yaramayabilir. Bu nedenle, en iyi $w$ ve $b$ deÄŸerlerini otomatik olarak bulan etkili bir algoritma olan "gradient descent (dereceli azalma)" algoritmasÄ±nÄ±n kullanÄ±lmasÄ± daha uygundur. Bu algoritma, J cost fonksiyonunu minimize etmeye yardÄ±mcÄ± olur. Gradient descent ve farklÄ± varyasyonlarÄ±, yalnÄ±zca lineer regresyon eÄŸitiminde deÄŸil, dÃ¼nyanÄ±n en bÃ¼yÃ¼k ve karmaÅŸÄ±k yapay zeka modellerinde bile kullanÄ±lÄ±r.

### Gradient Descent

Gradient Descent (GD), Ã§ok Ã§eÅŸitli problemlerde en uygun Ã§Ã¶zÃ¼mleri bulma yeteneÄŸine sahip genel bir optimizasyon algoritmasÄ±dÄ±r. GD'nin temel fikri, bir maliyet fonksiyonunu en aza indirmek iÃ§in parametreleri tekrarlayan ÅŸekilde gÃ¼ncellemektir.

Hayal edin ki yoÄŸun bir sisin iÃ§inde daÄŸlarda kayboldunuz. Sadece ayaklarÄ±nÄ±zÄ±n altÄ±ndaki zeminin eÄŸimini hissedebilirsiniz. HÄ±zla vadinin dibine inmek isterseniz, en dik yokuÅŸun yÃ¶nÃ¼nde aÅŸaÄŸÄ± inmek mantÄ±klÄ± olur. Ä°ÅŸte bu durum, Gradient Descent'in temel prensibini aÃ§Ä±klar: $w$ parametre vektÃ¶rÃ¼ne gÃ¶re hata fonksiyonunun yerel gradyanÄ±nÄ± Ã¶lÃ§er ve azalan gradyan yÃ¶nÃ¼nde adÄ±m atar. EÄŸimin sÄ±fÄ±r olduÄŸu noktada, minimuma ulaÅŸÄ±lmÄ±ÅŸ olur. Yani $J_{\text{min}}(w,b)$ elde edilir.

Gradient Descent formÃ¼lÃ¼ ÅŸu ÅŸekildedir:

$$w:=w-a {d\over dw}j(w,b)$$

$$b:=b-a {d\over db}j(w,b)$$

Burada "$:=$" iÅŸareti, sol taraftaki "$w$" parametresinin, saÄŸ tarafÄ±ndaki "$w - a \frac{d}{dw}j(w,b)$" ifadesine eÅŸitlendiÄŸini gÃ¶sterir. Bu formÃ¼l, $w$ parametresini gÃ¼ncellemek iÃ§in kullanÄ±lÄ±r.

"$a$" parametresi, Yunan alfabesinin alfa harfini temsil eder. Bu denklemde, alfa genellikle 0 ile 1 arasÄ±nda bir pozitif sayÄ±dÄ±r, Ã¶rneÄŸin $0.01$ gibi. Alfa, ne kadar bÃ¼yÃ¼k adÄ±mlar atacaÄŸÄ±nÄ±zÄ± belirler. BÃ¼yÃ¼k bir alfa, daha hÄ±zlÄ± bir yokuÅŸ aÅŸaÄŸÄ± inmeye neden olurken, kÃ¼Ã§Ã¼k bir alfa daha dikkatli ve yavaÅŸ adÄ±mlar atmaya yol aÃ§ar.

"$\frac{d}{dw}j(w,b)$", $j(w,b)$ fonksiyonunun $w$ parametresine gÃ¶re tÃ¼revidir. Bu tÃ¼rev, hangi yÃ¶ne doÄŸru adÄ±m atÄ±lmasÄ± gerektiÄŸini belirler.

"$j(w,b)$" ise maliyet fonksiyonunu temsil eder. Bu fonksiyon, modelin veri kÃ¼mesindeki hatalarÄ± Ã¶lÃ§er ve modelin doÄŸruluÄŸunu artÄ±rmak iÃ§in kullanÄ±lÄ±r. Bu fonksiyon, modelin $w$ ve $b$ parametrelerine baÄŸlÄ± olarak ifade edilir.

Gradient Descent algoritmasÄ±, modelinizi daha iyi bir uyum saÄŸlamak Ã¼zere otomatik olarak gÃ¼ncellemeye yardÄ±mcÄ± olur. Bu sayede en iyi $w$ ve $b$ parametrelerini bulabilir ve maliyeti minimuma indirebilirsiniz. Bu algoritma, sadece lineer regresyon gibi temel modellerde deÄŸil, daha karmaÅŸÄ±k yapay zeka modellerinde de yaygÄ±n olarak kullanÄ±lÄ±r.

#### KÄ±smi tÃ¼revini alma

KÄ±smi tÃ¼rev, bir fonksiyonun birden fazla deÄŸiÅŸkeni olduÄŸunda, sadece belirli bir deÄŸiÅŸkenin tÃ¼revidir. Gradient Descent yÃ¶ntemi, bu kÄ±smi tÃ¼revlerin kullanÄ±mÄ±nÄ± iÃ§erir ve bir fonksiyonun minimumunu veya maksimumunu bulmada yardÄ±mcÄ± olur.

Bu Ã¶rnekte saÄŸ ve sol yÃ¶nlerdeki gradient descent iÅŸlemlerini ele alalÄ±m:

**SaÄŸa DoÄŸru Gradient Descent:**

![right_gradient](/static/right_gradient.png)

Daha Ã¶nce $b$ parametresini geÃ§ici olarak 0 olarak kabul ettiÄŸimizi hatÄ±rlayalÄ±m. Bu durumda sadece $w$ parametresi bulunuyor. Eksenlerde yatayda $w$ parametresi, dikeyde ise $J(w)$ maliyet fonksiyonu yer alÄ±r. Dereceli azalmayÄ± rastgele bir $w$ deÄŸeriyle baÅŸlatalÄ±m ve bu noktadan $J$ fonksiyonuna doÄŸru bir Ã§izgi Ã§izelim. Dereceli azalma, $w$ deÄŸerini eski $w$ deÄŸerinden, alfa deÄŸeri Ã§arpÄ± tÃ¼rev ifadesinden Ã§Ä±kararak gÃ¼nceller. Bu tÃ¼rev ifadesi, $J(w)$ fonksiyonunun o noktadaki eÄŸiminin negatifini temsil eder. Yani, bu Ã§izgi, eÄŸrinin o noktadaki teÄŸetini temsil eder. EÄŸer teÄŸet Ã§izgisi saÄŸa yukarÄ±ya doÄŸru ilerliyorsa, eÄŸim pozitif demektir (tÃ¼rev pozitif bir sayÄ±dÄ±r). Bu durumda, gÃ¼ncellenen $w$ deÄŸeri, $w - \alpha$ pozitif bir sayÄ± olur. EÄŸim pozitif olduÄŸundan, sola doÄŸru hareket ediyor ve $w$ deÄŸerini azaltÄ±yoruz. AmacÄ±mÄ±z $J$ deÄŸerini azaltmak olduÄŸundan, bu adÄ±m doÄŸru bir yÃ¶nde atÄ±lÄ±yor gibi gÃ¶rÃ¼nÃ¼yor.

**Sola DoÄŸru Gradient Descent:**

![left_gradient](/static/left_gradient.png)

AynÄ± mantÄ±ÄŸÄ± bir adÄ±m geriye giderek dÃ¼ÅŸÃ¼nelim. Bu kez $w$ parametresini sol tarafta bir baÅŸlangÄ±Ã§ deÄŸeri seÃ§elim. Bu, $J$ fonksiyonunun o noktasÄ±na karÅŸÄ±lÄ±k gelir. TÃ¼rev ifadesi $dJ(w)/dw$ olup, bu noktadaki teÄŸet Ã§izgisine bakarsak, eÄŸim negatif olacaktÄ±r. Bu durumda, $w$'yi gÃ¼ncellediÄŸimizde, $w - \alpha$ negatif bir sayÄ± olur. Bu da $w$ deÄŸerini artÄ±rÄ±r, Ã§Ã¼nkÃ¼ negatif bir sayÄ±dan Ã§Ä±karmak, pozitif bir sayÄ± eklemekle aynÄ±dÄ±r. Dereceli azalmanÄ±n bu adÄ±mÄ±, $J$ fonksiyonunun azaldÄ±ÄŸÄ± saÄŸa doÄŸru bir yÃ¶nde hareketi temsil eder.

SonuÃ§ olarak, dereceli azalma yÃ¶ntemi, fonksiyonun minimumunu veya maksimumunu bulma sÃ¼recinde mantÄ±klÄ± adÄ±mlar atmayÄ± saÄŸlar. SaÄŸa veya sola doÄŸru gradient descent, fonksiyonun eÄŸiminin pozitif veya negatif olduÄŸu noktalarda adÄ±m atarak hedefe yaklaÅŸmayÄ± amaÃ§lar. Bu yÃ¶ntem, modelin parametrelerini gÃ¼ncellemek ve maliyet fonksiyonunu minimize etmek iÃ§in yaygÄ±n olarak kullanÄ±lÄ±r.

#### Ã–ÄŸrenme OranÄ± (Learning Rate)

Bir modeli eÄŸitirken, Gradient Descent yÃ¶ntemini kullanarak parametreleri gÃ¼ncellemek ve maliyet fonksiyonunu minimuma yaklaÅŸtÄ±rmak amacÄ±yla adÄ±mlar atÄ±lÄ±r. Bu adÄ±mlarÄ±n bÃ¼yÃ¼klÃ¼ÄŸÃ¼nÃ¼ belirleyen Ã¶nemli bir parametre, Ã¶ÄŸrenme oranÄ±dÄ±r.

![gradian_inisi](/static/gradian_inisi.png)

Ã–ÄŸrenme oranÄ±, algoritmanÄ±n her iterasyonda ne kadar bÃ¼yÃ¼k bir adÄ±m atacaÄŸÄ±nÄ± belirleyen hiperparametredir. Ã–ÄŸrenme oranÄ± Ã§ok kÃ¼Ã§Ã¼kse, algoritma yakÄ±nsamak iÃ§in Ã§ok fazla iterasyon gerektirecektir, bu da eÄŸitim sÃ¼resini uzatabilir.

![gradient_small](/static/gradient_small.png)

Ã–te yandan, Ã¶ÄŸrenme oranÄ± Ã§ok yÃ¼ksekse, algoritma vadinin diÄŸer tarafÄ±na atlayabilir ve hatta daha Ã¶nce bulunandan daha yÃ¼kseÄŸe Ã§Ä±kabilir. Bu, algoritmanÄ±n yakÄ±nsamayÄ± atlamasÄ±na ve istenmeyen sonuÃ§lara yol aÃ§abilir.

![gradient_large](/static/gradient_large.png)

AyrÄ±ca, tÃ¼m maliyet fonksiyonlarÄ± dÃ¼z bir "kase" ÅŸeklinde olmayabilir. Ã‡ukurlar, sÄ±rtlar, platolar gibi Ã§eÅŸitli topografyalara sahip olabilirler. Åekil 4-6, Gradient Descent'in iki temel zorluÄŸunu gÃ¶sterir: Rastgele baÅŸlatÄ±lan bir algoritma soldan baÅŸlarsa, yerel minimuma doÄŸru yakÄ±nsar. SaÄŸdan baÅŸlarsa, dÃ¼z bir plato Ã¼zerinde Ã§ok uzun bir sÃ¼re geÃ§irebilir ve asla global minimuma ulaÅŸamayabilir.

![gradient__](/static/gradient__.png)

Neyse ki, DoÄŸrusal Regresyon modeli iÃ§in MSE maliyet iÅŸlevi dÄ±ÅŸbÃ¼key bir yapÄ±ya sahiptir; bu, eÄŸri Ã¼zerinde herhangi iki noktayÄ± seÃ§tiÄŸinizde, bu noktalarÄ± birleÅŸtiren doÄŸru parÃ§asÄ±nÄ±n eÄŸriyi asla geÃ§meyeceÄŸi anlamÄ±na gelir. Bu, yerel minimumlarÄ±n olmadÄ±ÄŸÄ±, yalnÄ±zca bir kÃ¼resel minimum olduÄŸu anlamÄ±na gelir. AynÄ± zamanda, eÄŸimi aniden deÄŸiÅŸmeyen sÃ¼rekli bir fonksiyondur. Bu iki gerÃ§eÄŸin bÃ¼yÃ¼k bir sonucu vardÄ±r: Gradient Descent, global minimuma ulaÅŸmada rastgele yaklaÅŸmayÄ± garanti eder (yeterince uzun sÃ¼re beklerseniz ve Ã¶ÄŸrenme oranÄ± Ã§ok yÃ¼ksek deÄŸilse).

AslÄ±nda, maliyet fonksiyonu bir "kase" ÅŸeklindedir, ancak Ã¶zellikler farklÄ± Ã¶lÃ§eklere sahipse, bu kase uzun bir vadisi ÅŸeklini alabilir. Åekil 4-7, Ã¶zellik 1 ve Ã¶zellik 2'nin aynÄ± Ã¶lÃ§eÄŸe sahip olduÄŸu bir eÄŸitim setinde (solda) ve Ã¶zellik 1'in Ã¶zellik 2'den Ã§ok daha kÃ¼Ã§Ã¼k deÄŸerlere sahip olduÄŸu bir eÄŸitim setinde (saÄŸda) Gradient Descent'i gÃ¶stermektedir.

![[gradient_y|500]]

![gradient_y](/static/gradient_y.png)

GÃ¶rdÃ¼ÄŸÃ¼nÃ¼z gibi, soldaki Gradient Descent algoritmasÄ± doÄŸrudan minimuma gidiyor ve bu nedenle hÄ±zla ulaÅŸÄ±yor. SaÄŸda ise Ã¶nce global minimuma neredeyse dik bir yÃ¶nde ilerliyor ve daha sonra uzun bir yolculukla sona eriyor. Neredeyse dÃ¼z bir vadiden aÅŸaÄŸÄ± iniyor. Sonunda minimum seviyeye ulaÅŸacak, ancak bu uzun zaman alacaktÄ±r.

Bu ÅŸema ayrÄ±ca, bir modelin eÄŸitiminin, bir maliyet fonksiyonunu (eÄŸitim seti Ã¼zerinden) en aza indiren model parametrelerinin bir kombinasyonunu aramak anlamÄ±na geldiÄŸini gÃ¶sterir. Bu, model parametre uzayÄ±nda bir arama yapma iÅŸlemidir: Bir model ne kadar fazla parametreye sahipse, parametre uzayÄ± o kadar fazla boyuta sahip olur ve arama o kadar zorlaÅŸÄ±r. Ã–rneÄŸin, 300 boyutlu bir uzayda iÄŸne aramak, Ã¼Ã§ boyutlu bir uzayda iÄŸne aramaktan Ã§ok daha zordur. Neyse ki, Lineer Regresyon durumunda maliyet fonksiyonu dÄ±ÅŸbÃ¼key olduÄŸu iÃ§in, iÄŸne kase ÅŸeklinin en altÄ±ndadÄ±r.

#### DoÄŸru GÃ¼ncelleme

![update](/static/update.png)

**doÄŸru gÃ¼ncelleme:**

Ä°ÅŸte eÅŸ zamanlÄ± olarak gÃ¼ncelleyen doÄŸru bir dereceli azalma algoritmasÄ±. `tmp_w` ve `tmp_b` deÄŸiÅŸkenlerine saÄŸ taraftaki ifadeleri eÅŸitleyip gÃ¼ncelleriz. Daha sonra, `tmp_w` deÄŸerini w'ye, `tmp_b` deÄŸerini de b'ye atadÄ±k. Ancak burada dikkat edilmesi gereken nokta, w deÄŸerinin gÃ¼ncellenmemiÅŸ halinin kullanÄ±lmasÄ±dÄ±r. Bu durumda, gÃ¼ncellenmemiÅŸ w deÄŸeri, tÃ¼rev ifadesine de uygulanacaktÄ±r. Yani kÄ±sacasÄ± `tmp_b`, gÃ¼ncellenmemiÅŸ w ile gÃ¼ncellenecektir.

**yanlÄ±ÅŸ gÃ¼ncelleme:**

Burada `tmp_w` deÄŸiÅŸkenini saÄŸ taraftaki ifadeyle eÅŸitleyip gÃ¼ncelledik. Daha sonra gÃ¼ncellenen deÄŸeri w deÄŸiÅŸkenine atadÄ±k. `tmp_b` deÄŸiÅŸkeni ise saÄŸ taraftaki ifadeyle gÃ¼ncellendi, fakat yeni deÄŸer alan w ile gÃ¼ncellendiÄŸi iÃ§in yanlÄ±ÅŸ sonuÃ§ Ã¼retecektir.

Buradaki Ã¶nemli nokta, her iki deÄŸiÅŸkenin eÅŸ zamanlÄ± olarak gÃ¼ncellenip eÅŸ zamanlÄ± olarak yeni deÄŸerlere atanmasÄ±dÄ±r. Bu ÅŸekilde daha doÄŸru sonuÃ§lara ulaÅŸmÄ±ÅŸ oluruz.

Lineer Regresyon iÃ§in Gradient Descent:

$$w = w-a{1\over m}  \sum_{i=1}^{m}{(Å·^i - y^i)}x^i$$

$$b = b-a{1\over m}  \sum_{i=1}^{m}{(Å·^i - y^i)}$$

### Ã–rnek Kod

```py
import numpy as np

def cost_function(X, y, theta):
	# Maliyet fonksiyonunu hesaplayan fonksiyon
Â  Â  n = len(X)

Â  Â  y_hat = X.dot(theta)

Â  Â  squared_errors = (y_hat - y) ** 2

Â  Â  cost = 1 / (2 * n) * np.sum(squared_errors)

Â  Â  return cost
```

```py

def gradient_descent(X, y, theta, alpha, num_iterations):
	# Gradyan iniÅŸ algoritmasÄ± ile theta deÄŸerlerini gÃ¼ncelleyen fonksiyon
Â  Â  m = len(X)

Â  Â  costs = []

Â  Â  for _ in range(num_iterations):

Â  Â  Â  Â  predictions = X.dot(theta)

Â  Â  Â  Â  errors = predictions - y

Â  Â  Â  Â  gradient = (1 / m) * X.T.dot(errors)

Â  Â  Â  Â  theta -= alpha * gradient

Â  Â  Â  Â  cost = cost_function(X, y, theta)

Â  Â  Â  Â  costs.append(cost)

Â  Â  return theta, costs
Â  Â 
```

```py
def linear_regression(X, y, alpha, num_iterations):
	# DoÄŸrusal regresyon modelini oluÅŸturan fonksiyon
Â  Â  X = np.column_stack((np.ones(len(X)), X)) Â # X'in baÅŸÄ±na birler sÃ¼tunu ekle

Â  Â  theta = np.zeros(X.shape[1]) Â # BaÅŸlangÄ±Ã§ta tÃ¼m theta deÄŸerlerini sÄ±fÄ±r ata

Â  Â  theta, costs = gradient_descent(X, y, theta, alpha, num_iterations)

Â  Â  return theta, costs


X = np.array([1, 2, 3, 4, 5]) # BaÄŸÄ±msÄ±z deÄŸiÅŸken

y = np.array([2, 4, 6, 8, 10]) # BaÄŸÄ±mlÄ± deÄŸiÅŸken


alpha = 0.01 Â # Ã–ÄŸrenme oranÄ±

num_iterations = 1000 Â # Ä°terasyon sayÄ±sÄ±


theta, costs = linear_regression(X, y, alpha, num_iterations)


print("Theta:", theta)

print("Maliyetler:", costs)


```

```py

import matplotlib.pyplot as plt


# GerÃ§ek verilerin gÃ¶rselleÅŸtirilmesi

plt.scatter(X, y, color='blue', label='GerÃ§ek Veriler')


# Tahminlerin gÃ¶rselleÅŸtirilmesi

X_pred = np.array([0, 6])

y_pred = theta[0] + theta[1] * X_pred

plt.plot(X_pred, y_pred, color='red', label='DoÄŸrusal Regresyon Modeli')


plt.xlabel('X')

plt.ylabel('y')

plt.title('DoÄŸrusal Regresyon')

plt.legend()

plt.show()
```

![Ã¶rnk_kod](/static/Ã¶rnk_kod.png)

### sklearn ile Ã¶rnek kod

```py

# Gerekli kÃ¼tÃ¼phaneleri iÃ§e aktarÄ±n

import numpy as np

import matplotlib.pyplot as plt

from sklearn.linear_model import LinearRegression

# Ã–rnek veri setini oluÅŸturun

X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1) Â # BaÄŸÄ±msÄ±z deÄŸiÅŸkenler

y = np.array([2, 4, 6, 8, 10]) Â # BaÄŸÄ±mlÄ± deÄŸiÅŸkenler

```

```py

# Lineer regresyon modelini oluÅŸturun ve eÄŸitin

model = LinearRegression()

model.fit(X, y)

```

```py

# EÄŸitilmiÅŸ modelin katsayÄ±larÄ±nÄ± ve kesme noktasÄ±nÄ± alÄ±n

katsayilar = model.coef_

kesme_noktasi = model.intercept_

# GÃ¶rselleÅŸtirme iÃ§in tahminler yapÄ±n

y_pred = model.predict(X)

```

```py

# Veri noktalarÄ±nÄ± ve tahmin edilen doÄŸruyu gÃ¶rselleÅŸtirin

plt.scatter(X, y, color='blue', label='Veri NoktalarÄ±')

plt.plot(X, y_pred, color='red', linewidth=2, label='Tahmin Edilen DoÄŸru')

plt.xlabel('X')

plt.ylabel('Y')

plt.title('Basit DoÄŸrusal Regresyon')

plt.legend()

plt.show()

# SonuÃ§larÄ± yazdÄ±rÄ±n

print("KatsayÄ±lar:", katsayilar)

print("Kesme NoktasÄ±:", kesme_noktasi)
```

![Ã¶rn_kod_2](/static/Ã¶rn_kod_2.png)
